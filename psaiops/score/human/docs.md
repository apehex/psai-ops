## Overview

This tool estimates a "human score" for each token, a probability ranging from:

- `0` (in red): "definitely AI", the highest confidence that the token was generated by a LLM
- to `1` (in green): "definitely human", scoring the token that are most likely written by a human
- with `0.5` (in black): the neutral score, when the model cannot decide between the 2 options

To calculate this score, the input string provided by the user is processed as follows:

0. it is tokenized using `o200k-harmony` from OpenAI into a sequence of token IDs
1. these IDs are then transformed into raw logits by a forward pass in `openai/gpt-oss-20b`
2. the logits and indices are used to calculate raw indicators like the entropy
3. the indicators are log-scaled and mapped to `[0; 1]` probabilities
4. the tensors are padded and aligned with the tokens along the sequence axis
5. the probabilities are aggregated over a time window around each token
6. if enabled, the values are gradually dampened using a ramping function over the whole sample
7. all the indicators are combined into a single score in `[0; 1]` using a conflation function
8. the final score is expressed as a percentage in the UI (highlights and graphs)

## Assumptions And Limitations

### Context

Regardless of the author of the text, it was created within a specific context.
It is even possible that a LLM was prompted to "write like a human", greatly influencing the output.

A key assumption is that the context is embedded in the sample itself.
The further a LLM / human goes in a text sample, the more they are influenced by the sample itself rather than the original settings.

This is why the application has the option to dampen the first quarter of the sample:
the LLM critic doesn't have enough data yet to produce a reliable score.

### Generality

This tool is using an open source LLM as proxy for the proprietary models used in commercial chatbots.
Also, `gpt-oss-20b` was published in August 2025 and by the time you use this app it may be largely outdated.

The main assumption here is that the different LLMs share a similar model of text because the language itself is mostly stable.
Of course, this hypothesis has many sharp edges: the culture evolves rapidly, new languages emerge each year (especially in programming), less common dialects are progressively integrated, etc.

In short, this tool is most reliable on common languages and contexts, and you should use it with a critical eye in other settings (code, non western language, etc).

## Conflation Of Probabilities

Each of the calculated metrics evaluates the input text from a different angle.

These evidence are combined with a conflation function, which has specific properties.
Given a set of probabilities $\{ p\_{j} \}$, adding another probability $p$ will:

- leave the conflation score unchanged if $p = 0.5$: $C(\{ p\_{j}, 0.5 \}) = C(\{ p\_{j} \})$
- increase the score if $p > 0.5$: $C(\{ p\_{j}, p \}) > C(\{ p\_{j} \})$
- decrease the score otherwise

The conflation function is defined by:

$$\begin{align}
C(\{ p\_{j} \}) = \frac{\Prod\_{j=1}\^{M} p\_{j}}{(\Prod\_{j=1}\^{M} p\_{j}) + \Prod\_{j=1}\^{M} (1 - p\_{j})}
\end{align}$$

## Indicators And Metrics

### Unicode

Since we use constrained interfaces to type text (keyboard, physical or virtual), everyday writers use a very limited range of Unicode glyphs.
Hence, typical human text has many trade-offs between strict correctness and visual approximations.

For example, in French the apostrophe is used when contracting several words.
But it is not as readily available on a keyboard as the ASCII "'": people usually write "l'app" and not "lâ€™app".
The later form occurs either in well-made and meticulously edited human text or most often when a LLM output that was just copy-pasted.

Since these Unicode glyphs are sometimes hard to spot visually, they are scored "0" for "definitely AI" so that they are clearly highlighted by a red background.
As you will see, this is often wrong (for example with emojis) but also an obvious mistake you will naturally discard.

### Surprisal

First the whole distribution of logits is reduced into its entropy:

$$\begin{align}
H\_{t} = \Sum\_{i=1}\^{i=N} - P(x\_{t} = V(i)| X\_{< t}) \log P(x\_{t} = V(i)| X\_{< t})
\end{align}$$

This indicator measures the spread of the probability distribution, in other words how uncertain the model is about each token prediction.

The entropy of all the token possibilities is then compared with the likelihood of the token actually in the sample:

$$\begin{align}
\Delta\_{t} = - \log P(x\_{t} = X\_{t} | X\_{< t}) - H\_{t}
\end{align}$$

And this difference is rescaled and centered into the final surprisal metric:

$$\begin{align}
S\_{t} = 0.5 + \frac{\Delta\_{t}}{\log V}
\end{align}$$

So when the expectations and realizations agree the delta is null and the surprisal has the neutral value $0.5$.
The more surprised the model is the more "human" the token is scored, and the more obvious a token is the more "AI" it is scored.

Since the spikes in this curve are meaningful evidence, they are preserved during the aggregation (over time) thanks to a top-k pooling:

### Perplexity

The surprisal gives a high frequency metric and it is balanced with a low frequency average, the simple negative log likelihoods:

$$\begin{align}
NLL\_{t} = E\_{j \in \[t-W; t+W\]} - \log P(x\_{j} = X\_{j} | X\_{< j})
\end{align}$$

Having observed this indicator on various inputs, I decided that an average NLL of $\log 40$ was a decent estimation for the neutral balance between AI / human.
Then I rescaled the NLL accordingly to calculate the perplexity metric:

$$\begin{align}
PP\_{t} = \frac{NLL\_{t} - \log 2}{\log 800 - \log 2}
\end{align}$$

This is obviously not the "canon" perplexity of information theory, just a connex function that makes sense in the context of this app.

## Notations

| Symbol                                                                    | Meaning                                                                           |
| ------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |
| $V$                                                                       | The vocabulary, a collection of tokens used by the models                         |
| $N = | V |$                                                               | The size of the vocabulary                                                        |
| $X$                                                                       | The input text sample, as a sequence of tokens                                    |
| $T = | X |$                                                               | The length of the input, counted in tokens                                        |
| $i$, $j$, $k$                                                             | Generic indices, spanning the vocabulary or other axes                            |
| $t$                                                                       | Another indice dedicated to the time axis / token position                        |
| $x\_{t}$                                                                  | The token at position $t$ in $X$, either meant as a token string or index         |
| $P(x\_{t} = V(i)| x\_{< t})$                                              | The probability that the next token is $V(i)$, estimated by the LLM               |

## TODO / Improvements

[ ] add a few representative samples to show the behavior of the model and scores
[ ] generate variants of the input prompts with small perturbations:
    [ ] a batch with small syntactic variations that don't affect the meaning
    [ ] another batch with conservative rewording (expansion and contraction of words, synonyms, etc)
